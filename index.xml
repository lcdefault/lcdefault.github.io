<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chengkun Wei</title><link>https://lcdefault.github.io/</link><atom:link href="https://lcdefault.github.io/index.xml" rel="self" type="application/rss+xml"/><description>Chengkun Wei</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-US</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate><image><url>https://lcdefault.github.io/media/icon_hu3806124278901945205.png</url><title>Chengkun Wei</title><link>https://lcdefault.github.io/</link></image><item><title>喜讯！浙江大学ARClab实验室参与科创成果荣获浙江省科学技术进步一等奖</title><link>https://lcdefault.github.io/post/2024-science-tech/</link><pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate><guid>https://lcdefault.github.io/post/2024-science-tech/</guid><description>&lt;p>2024 年 11 月 22 日，全省科学技术奖励大会在杭州举行，2023 年度浙江省科学技术奖正式公布。浙江大学 ARClab 实验室做为主要单位参与的“云网端新型融合计算架构及应用”项目 荣获浙江省科学技术进步奖一等奖。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="image" srcset="
/post/2024-science-tech/featured_hu16988964722115084523.webp 400w,
/post/2024-science-tech/featured_hu4975215701303551261.webp 760w,
/post/2024-science-tech/featured_hu2431804484886379588.webp 1200w"
src="https://lcdefault.github.io/post/2024-science-tech/featured_hu16988964722115084523.webp"
width="760"
height="250"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>浙江省科学技术奖是浙江省内科技领域的最高荣誉，主要包括浙江科技大奖、自然科学奖、技术发明奖、科学技术进步奖、国际科学技术合作奖五大奖别。其中，科学技术进步奖的颁发对象为完成并应用推广创新性科学技术成果，为推动科学技术进步和经济社会发展作出贡献的单位和个人。&lt;/p></description></item><item><title>喜讯！浙江大学ARClab实验室参与科创成果荣获浙江省科学技术进步一等奖</title><link>https://lcdefault.github.io/post/2024-science-tech2/</link><pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate><guid>https://lcdefault.github.io/post/2024-science-tech2/</guid><description>&lt;p>2024 年 11 月 22 日，全省科学技术奖励大会在杭州举行，2023 年度浙江省科学技术奖正式公布。浙江大学 ARClab 实验室做为主要单位参与的“云网端新型融合计算架构及应用”项目 荣获浙江省科学技术进步奖一等奖。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="image" srcset="
/post/2024-science-tech2/featured_hu16988964722115084523.webp 400w,
/post/2024-science-tech2/featured_hu4975215701303551261.webp 760w,
/post/2024-science-tech2/featured_hu2431804484886379588.webp 1200w"
src="https://lcdefault.github.io/post/2024-science-tech2/featured_hu16988964722115084523.webp"
width="760"
height="250"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>浙江省科学技术奖是浙江省内科技领域的最高荣誉，主要包括浙江科技大奖、自然科学奖、技术发明奖、科学技术进步奖、国际科学技术合作奖五大奖别。其中，科学技术进步奖的颁发对象为完成并应用推广创新性科学技术成果，为推动科学技术进步和经济社会发展作出贡献的单位和个人。&lt;/p></description></item><item><title>喜讯！ARClab在国际顶级会议NeurIPS官方赛道大语言模型隐私挑战赛（LLM-PC）创佳绩</title><link>https://lcdefault.github.io/post/2024-llm-pc/</link><pubDate>Sat, 01 Jun 2024 13:00:00 +0000</pubDate><guid>https://lcdefault.github.io/post/2024-llm-pc/</guid><description>&lt;p>近日，由浙江大学计算机体系结构实验室（ARClab）和蚂蚁链摩斯组成的参赛队伍“Morse &amp;amp; ARClab”荣获第三十八届神经信息处理系统年会（NeurIPS 2024）特设官方赛——大语言模型隐私挑战赛（LLM-PC）全部两个赛道中攻击赛道的冠军和一个最佳实用防御奖。&lt;/p>
&lt;h2 id="赛事介绍">赛事介绍&lt;/h2>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="image" srcset="
/post/2024-llm-pc/neurips-2024_hu8401591888294272640.webp 400w,
/post/2024-llm-pc/neurips-2024_hu6528143322849976394.webp 760w,
/post/2024-llm-pc/neurips-2024_hu4405046113806629847.webp 1200w"
src="https://lcdefault.github.io/post/2024-llm-pc/neurips-2024_hu8401591888294272640.webp"
width="760"
height="291"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>NeurIPS 是机器学习领域三大旗舰会议之一，也是中国计算机学会 A 类推荐会议。大语言模型隐私挑战赛（LLM-PC）是本届 NeurIPS 的特设官方竞赛。竞赛聚焦大语言模型训练数据的隐私安全。本次竞赛由加州大学伯克利分校、德州大学奥斯汀分校、伊利诺伊大学厄巴纳-香槟分校、新加坡国立大学、Center for AI Safety 等高校与机构联合命题组织，吸引了来自加州大学伯克利分校、芝加哥大学、微软、亚马逊等 30 支队伍参与角逐，汇聚全球顶尖的研究人员、开发人员和从业人员，共同面对解决人工智能隐私保护领域面临的重大挑战。&lt;/p>
&lt;p>参赛者的任务是设计与实现创新解决方案，从下游模型中窃取训练数据中的隐私或者设计隐私保护训练方法。竞赛不仅能够激发技术创新，还将促进业界对人工智能安全影响的深入理解，有助于推动整个领域向构建更加安全、可靠的 AI 系统目标前进。&lt;/p>
&lt;h2 id="赛道介绍">赛道介绍&lt;/h2>
&lt;p>LLM-PC 竞赛于 2024 年 7 月底开始，设由红色和蓝色两个赛道，红色赛道要求参赛者设计针对大模型的训练数据窃取方法，蓝色赛道要求参赛者设计隐私保护方法。两个赛道都要求提供的算法可以 24 小时之内 3 块 H100 上完成。&lt;/p>
&lt;h3 id="赛道一red-team-track">赛道一：Red-Team Track&lt;/h3>
&lt;p>红队成员的任务是发现大型语言模型中可能泄露敏感信息的漏洞。这一任务直接与行业面临的防止可能导致重大财务和声誉损失的数据泄露挑战相关联。一个实际的例子是从在私有领域数据（如金融数据）上微调的模型中提取个人可识别信息（PII）。参赛者需要在比赛方提供的使用隐私数据微调的 Llama-3.1-8B 模型上提取 PII 数据。红队赛道根据 PII 数据的准确度排名。&lt;/p>
&lt;h3 id="赛道二blue-team-track">赛道二：Blue-Team Track&lt;/h3>
&lt;p>蓝队成员则专注于加强大型语言模型对隐私攻击的防御，提升它们保护数据的能力。这反映了学术界和工业界创造尊重用户隐私的安全人工智能系统的追求。一个应用场景可能包括设计方法来清洗对大型语言模型的健康相关查询的输出，确保不会无意中泄露敏感的患者信息。参赛者根据提供的模型和数据设计防御方法抵御红队的攻击。&lt;/p>
&lt;h2 id="方法介绍">方法介绍&lt;/h2>
&lt;h3 id="red-team-track">Red-Team Track&lt;/h3>
&lt;p>我们的攻击方法包括两个主要步骤。首先，我们通过查询目标模型来构建提示语，促使其为每个掩盖的个人可识别信息（PII）生成候选回应。接下来，我们计算每个生成候选插入原文后，整个句子的损失值。然后，我们基于损失的贪心搜索，选择损失值最低的候选作为最终输出。我们设计了一种高效的基于分段与聚合的损失计算方法，以加速候选项的选择过程。我们的实验表明，我们的方法在比赛提供的 Llama3.1-8B 模型上可以达到 0.233 的攻击成功率。&lt;/p>
&lt;h3 id="blue-team-track">Blue-Team Track:&lt;/h3>
&lt;p>我们的防御方法包括两个主要步骤。首先，我们对包含个人可识别信息（PII）的语料进行随机替换，这一步中我们会对每种 PII（例如姓名，地址等）从对应候选集合中随机选取一个词进行替换，其中候选词是通过访问外部大模型获取的。之后，我们使用替换后的语料以自回归的方式微调目标模型。通过这样的方式，可以扰乱大模型对 PII 信息的记忆，降低其输出正确 PII 的可能。我们的方案对模型推理时间几乎没有影响，可以使主办法提供的攻击方法成功率相对降低 30.6%，同时在 MMLU，truthfulqa 等基准数据集上的结果也表明我们的方法对模型性能几乎没有损耗。&lt;/p>
&lt;h2 id="团队介绍">团队介绍&lt;/h2>
&lt;p>获奖成员来自浙江大学计算机体系结构实验室（ARClab）与蚂蚁链摩斯。其中参赛学生为孟文龙（ARClab 博士生）、郭镇远（ARClab 博士生）、吴乐南、杨勇（ARClab 博士生），蚂蚁方面参赛人员为刘文炎，李唯贤（ARClab 毕业生），殷山。指导老师为陈文智教授、魏成坤研究员。&lt;/p></description></item><item><title>喜讯！ARClab团队论文连续3年被HPCA录用</title><link>https://lcdefault.github.io/post/2024-hpca/</link><pubDate>Wed, 01 May 2024 13:00:00 +0000</pubDate><guid>https://lcdefault.github.io/post/2024-hpca/</guid><description>&lt;p>浙江大学计算机系统结构实验室陈义全、靳珍的论文“NVMePass: A Lightweight, High-performance and Scalable NVMe Virtualization Architecture with I/O Queues Passthrough”被体系结构领域CCF-A类会议HPCA’25录用。该论文由陈文智教授指导，提出了一种轻量化、高性能和可扩展的新型NVMe存储虚拟化架构。其核心思想是将NVMe I/O硬件队列直通给虚拟机，同时在NVMe SSD控制器Firmware里实现NRD（NVMe Resource Domain）机制以拦截非法I/O请求，实现安全隔离。该虚拟化架构具备了硬件加速卸载方案的高性能，同时有软件虚拟化方案的灵活性。&lt;/p>
&lt;h2 id="会议介绍">会议介绍&lt;/h2>
&lt;p>High Performance Computer Architecture（HPCA）是由IEEE举办的计算机体系结构/高性能计算领域最重要的学术会议之一，与ASPLOS, ISCA, MICRO并称为计算机体系结构领域的“四大顶会”。&lt;/p>
&lt;h2 id="现有nvme存储虚拟化方案面临的挑战">现有NVMe存储虚拟化方案面临的挑战&lt;/h2>
&lt;p>目前主流的存储虚拟化技术可以分为软件方案和硬件辅助两大类。软件方案通过主机软件将NVMe设备虚拟化，而无需在硬件上增加额外的功能。然而，现有的软件型方案存在严重的性能下降或CPU开销过大的问题。为了解决性能问题，出现了SPDK vhost-NVMe等基于轮询的方案，利用独立的CPU核来处理I/O请求来提高性能，但需要消耗的宝贵CPU资源。硬件加速方案在硬件层面复制PCIe功能，为不同的VM呈现NVMe设备，使得VM能够绕过主机软件栈，直接访问硬件资源。因此，这些方案可以在不消耗额外CPU资源的情况下实现高性能。 然而，硬件辅助解决方案需要专用硬件进行NVMe虚拟化，这会带来额外的成本和硬件设计复杂性。&lt;/p>
&lt;h2 id="新型nvme-io队列直通虚拟化架构">新型NVMe I/O队列直通虚拟化架构&lt;/h2>
&lt;p>本文提出了一种新型轻量级、软硬件协同设计的NVMe 直通虚拟化架构NVMePass，旨在实现高性能、零CPU开销，同时保持高可扩展性。NVMePass的关键思想是虚拟机的NVMe I/O队列直通和安全隔离机制。我们设计了一个新的I/O队列直通框架，使虚拟机能够直接访问硬件I/O队列，避免了软件虚拟化的高CPU开销挑战。为了解决直通后安全性的挑战，我们提出了一种NRD机制来拦截非法的 NVMe I/O请求。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img alt="image" srcset="
/post/2024-hpca/arch_hu5810075098327479792.webp 400w,
/post/2024-hpca/arch_hu11192383305755329687.webp 760w,
/post/2024-hpca/arch_hu18041553871359028056.webp 1200w"
src="https://lcdefault.github.io/post/2024-hpca/arch_hu5810075098327479792.webp"
width="711"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>NVMePass有两个关键而独特的设计：(1) I/O 队列直通框架，(2) 直通安全隔离机制。队列直通实现原理，是通过在虚拟化NVMe设备中创建虚拟控制器内存缓冲区 (CMB) 和自定义页面错误处理程序来实现VM和物理设备的I/O队列之间建立直接地址映射。安全隔离是在NVMe控制器固件中实现NRD资源域机制，以拦截非法的NVMe I/O请求。NVMePass提供接近原生硬件的性能。除了I/O队列直通之外，NVMePass还支持直接内存访问 (DMA) 和VM中断重映射，而无需虚拟机管理程序参与，从而消除了虚拟化开销。实验结果表明，NVMePass可以提供与VFIO相当的性能，IOPS可达VFIO的 100.1%-100.5%。此外，与SPDK-Vhost相比，NVMe在运行150台虚拟机时延迟降低了40.0%，在实际应用中运行100台虚拟机时OPS性能提升了68.0%。&lt;/p>
&lt;h2 id="作者介绍">作者介绍&lt;/h2>
&lt;p>论文作者陈义全、靳珍为浙江大学计算机系统结构实验室（ZJU ARClab）的博士生，主要研究方向为智能计算系统架构。&lt;/p></description></item><item><title>CMDRL: A Markovian Distributed Rate Limiting Algorithm in Cloud Networks</title><link>https://lcdefault.github.io/publication/10-1145-3663408-3663417/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://lcdefault.github.io/publication/10-1145-3663408-3663417/</guid><description/></item><item><title>Triton: A Flexible Hardware Offloading Architecture for Accelerating Apsara vSwitch in Alibaba Cloud</title><link>https://lcdefault.github.io/publication/10-1145-3651890-3672224/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://lcdefault.github.io/publication/10-1145-3651890-3672224/</guid><description/></item></channel></rss>